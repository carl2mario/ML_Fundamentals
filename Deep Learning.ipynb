{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Deep Learning Fundamentals`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllabus\n",
    "\n",
    "`Module 1 - Introduction to Deep Learning`\n",
    "- Introduction to Deep Learning\n",
    "- Biological Neural Networks\n",
    "- Artificial Neural Networks - Forward Propagation\n",
    "\n",
    "`Module 2 - Artificial Neural Networks`\n",
    "- Gradient Descent\n",
    "\n",
    "`Module 3 - Deep Learning Libraries`\n",
    "- Introduction to Deep Learning Libraries\n",
    "- Regression Models with Keras\n",
    "- Classification Models with Keras\n",
    "\n",
    "`Module 4 - Deep Learning Models`\n",
    "- Shallow and Deep Neural Networks\n",
    "- Convolutional Neural Networks\n",
    "- Recurrent Neural Networks\n",
    "- Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T08:31:08.416193Z",
     "start_time": "2018-12-24T08:31:08.402237Z"
    }
   },
   "source": [
    "------\n",
    "`Neural networks are universal function approximators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.458593Z",
     "start_time": "2018-12-25T00:46:35.387701Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png' width=70%>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/1200px-Colored_neural_network.svg.png' width=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.476080Z",
     "start_time": "2018-12-25T00:46:35.465014Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='https://i.stack.imgur.com/LgmYv.png' width=70%>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='https://i.stack.imgur.com/LgmYv.png' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Forward Propogation \n",
    "\n",
    "Process through which information propogates from input layer to output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. Start with the input layer as the input to the first hidden layer.\n",
    "2. Compute the weighted sum at the nodes of the current layer.\n",
    "3. Compute the output of the nodes of the current layer.\n",
    "4. Set the output of the current layer to be the input to the next layer.\n",
    "5. Move to the next layer in the network.\n",
    "5. Repeat steps 2 - 4 until we compute the output of the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "### Perceptron\n",
    "\n",
    "Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks. Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png' width=70%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Keras Sequential Network with Dense Layers Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.778193Z",
     "start_time": "2018-12-25T00:46:35.483017Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-88d96843a926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.781113Z",
     "start_time": "2018-12-25T00:46:35.360Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.786939Z",
     "start_time": "2018-12-25T00:46:35.366Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('metal_price.csv',index_col='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.789417Z",
     "start_time": "2018-12-25T00:46:35.375Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "target = df['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.789417Z",
     "start_time": "2018-12-25T00:46:35.381Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a=[]\n",
    "for i in df.columns:\n",
    "    if i!='Price':\n",
    "        a.append(i)\n",
    "predictors = df[a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.796002Z",
     "start_time": "2018-12-25T00:46:35.386Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.801466Z",
     "start_time": "2018-12-25T00:46:35.393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Two models in Keras\n",
    "\n",
    "- Sequential \n",
    "- Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.803762Z",
     "start_time": "2018-12-25T00:46:35.399Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.811421Z",
     "start_time": "2018-12-25T00:46:35.406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_columns=predictors.shape[1]\n",
    "model.add(Dense(5,activation='relu', input_shape=(n_columns,)))\n",
    "model.add(Dense(5,activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.816409Z",
     "start_time": "2018-12-25T00:46:35.413Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.819399Z",
     "start_time": "2018-12-25T00:46:35.421Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model.fit(predictors,target,epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-25T00:46:35.822172Z",
     "start_time": "2018-12-25T00:46:35.432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Activation functions introduce non-linearity to the networks that is why we call them non-linearities. Neural Networks are trained using backpropapagation which requires `differentiable` activation functions.\n",
    "\n",
    "If we do not apply a Activation function then the output signal would simply be a simple linear function. A Neural Network without Activation function would simply be a Linear regression Model. Also without activation function our Neural network would not be able to learn and model other complicated kinds of data such as images, videos , audio , speech etc. That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear, big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Types of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Identity / Linear`\n",
    "\n",
    "Problem with linear activation function is that its derivative is a constant and its gradient will be a constant too and the descent will be on a constant gradient.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*gklL4_EwFpXPSzFC4sPT1g.png'>\n",
    "* Derrivative\n",
    "<img src=''>\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/NKESX.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Step / Heaviside`\n",
    "\n",
    "is typically only useful within single-layer perceptrons, an early type of neural networks that can be used for classification in cases where the input data is linearly separable. These functions are useful for binary classification tasks.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*LfKVxBfSYSFyUwEw5YInFg.png'>\n",
    "\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/vRdzT.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Bipolar`\n",
    "\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/5tQJ9.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Piecewise Linear`\n",
    "\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/cguIH.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Sigmoid / Logistic Activation Function`\n",
    "\n",
    "Sigmoid non-linearity squashes real numbers to range between [0,1]. In particular, large negative numbers become 0 and large positive numbers become 1. It is mostly used for binary classification problems.\n",
    "\n",
    "It has two major drawbacks:\n",
    "* Sigmoids saturate and kill gradients (Vanishing Gradient Problem)\n",
    "\n",
    "If the local gradient is very small, it will effectively \"kill\" the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. \n",
    "\n",
    "Additionally, one must pay extra caution when initializing the weights of sigmoid neurons to prevent saturation. For example, if the initial weights are too large then most neurons would become saturated and the network will barely learn.\n",
    "\n",
    "* Sigmoid outputs are not zero-centered\n",
    "\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*Bhzlu8WmM1UFo8TltoRHOQ.png'>\n",
    "\n",
    "* Derrivative\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*Q_fCmWPcz4F8IoNXm9tqcg.png'>\n",
    "\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/COTWF.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Complementary log-log`\n",
    "\n",
    "aij=σ(zij)=1−exp(−exp(zij))\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/LcZHq.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Hyperbolic Tangent (Tanh)`\n",
    "\n",
    "The tanh non-linearity squashes real numbers to range between [-1,1]. It looks like a scaled sigmoid function. Data is centered around zero, so the derivatives will be higher. Tanh quickly converges than sigmoid and logistic activation functions. Downside is that it suffers from Vanishing Gradient Problem as well.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*rACT-asoF6gANDE2fW07IQ.png'>\n",
    "\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/gQ9zn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Absolute`\n",
    "\n",
    "aij=σ(zij)=∣zij∣\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/BADmK.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Rectifier / ReLU`\n",
    "\n",
    "Also known as Rectified Linear Unit (ReLU), Max, or the Ramp Function. It is zero when x < 0 and then linear with slope 1 when x > 0. It trains 6 times faster than tanh.\n",
    "\n",
    "It was found to greatly accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh functions. It is argued that this is due to its linear, non-saturating form.\n",
    "\n",
    "Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.\n",
    "\n",
    "Unfortunately, ReLU units can be fragile during training and can \"die\". For example, a large gradient flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any data-point again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. With a proper setting of the learning rate (not too high) this is less frequently an issue.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*ZH-D-NXMq82joIHyJocZ3w.png'>\n",
    "* Graph\n",
    "<img src='https://i.stack.imgur.com/a7hU1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Modified ReLU`\n",
    "\n",
    "* Leaky ReLU\n",
    "\n",
    "Leaky ReLUs are one attempt to fix the \"dying ReLU\" problem. Instead of the function being zero when x < 0, a leaky ReLU will instead have a small negative slope (of 0.01, or so).\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*qVrYlFchG7YiTX5WWx1r7A.png'>\n",
    "\n",
    "* Parametric Rectified Linear Unit (PReLU)\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*pZ5_JgEGDHEWsTFoVfK_2g.png'>\n",
    "\n",
    "* Randomized Leaky Rectified Linear Unit (RReLU)\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*2hEeawGNKs0WwGz9I0JqAg.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Exponential Linear Unit (ELU)`\n",
    "\n",
    "Exponential linear units try to make the mean activations closer to zero which speeds up learning. It has been shown that ELUs can obtain higher classification accuracy than ReLUs. α is a hyper-parameter that needs to be tuned.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*rc2g2ZIm4lRCNt8gWMhjyA.png'>\n",
    "\n",
    "* Graph\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*gfEr6eAKDZT8hHf2t7u7Lw.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`SoftPlus`\n",
    "\n",
    "The derivative of the softplus function is the logistic function. ReLU and Softplus are largely similar, except near 0(zero) where the softplus is enticingly smooth and differentiable. It's much easier and efficient to compute ReLU and its derivative than for the softplus function which has log(.) and exp(.) in its formulation.\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*EyVsonpsBRp5fdNa-djnBw.png'>\n",
    "\n",
    "* Derivative\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*D3YKEgImpixP1lst0_uljQ.png'>\n",
    "\n",
    "* Graph\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*w275Sin5bKAIaWBaJ6zXcA.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Maxout`\n",
    "\n",
    "A maxout layer is simply a layer where the activation function is the max of the inputs.\n",
    "\n",
    "The Maxout neuron computes the function max(wT1x+b1,wT2x+b2). Both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1=0). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU).\n",
    "\n",
    "However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Softmax`\n",
    "\n",
    "Also known as the Normalized Exponential. The softmax function is often used in the final layer of a neural network-based classifier. The output of a neuron is dependent on the other neurons in that layer. \n",
    "\n",
    "Softmax functions convert a raw value into a posterior probability. This provides a measure of certainty. It squashes the outputs of each unit to be between 0 and 1, just like a sigmoid function. But it also divides each output such that the total sum of the outputs is equal to 1.\n",
    "\n",
    "The output of the softmax function is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true\n",
    "\n",
    "* Function\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/1*XipIlq5eCmQMUDxr4PNH_g.png'>\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.\n",
    "\n",
    "ReLU and it’s variants should be preferred over sigmoid or tanh activation functions. As well as ReLUs are faster to train. If ReLU is causing neurons to be dead, use Leaky ReLUs or it’s other variants. Sigmoid and tanh suffers from vanishing gradient problem and should not be used in the hidden layers. ReLUs are best for hidden layers. Try tanh, but expect it to work worse than ReLU/Maxout.\n",
    "\n",
    "Activation functions which are easily differentiable and easy to train should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout - Regularization\n",
    "\n",
    "Loss function\n",
    "\n",
    "Feature Scaling\n",
    "\n",
    "Model Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of Neural Networks\n",
    "\n",
    "CNN\n",
    "\n",
    "RNN\n",
    "\n",
    "LSTM\n",
    "\n",
    "Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Algorithms\n",
    "\n",
    "Decsion Trees\n",
    "\n",
    "Random Forest\n",
    "\n",
    "K-Means\n",
    "\n",
    "K-Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
